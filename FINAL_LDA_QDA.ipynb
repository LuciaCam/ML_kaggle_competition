{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is statistical technique utilized for forecasting a single categorical variable based on one or more continuous variables, separating the samples into two or more classes that are based on distance through one or many decision boundaries. It is also utilized to determine the numeric association between sets of these variables. It enables us to determine the probability of an observation belonging to a specific class based on the predictor value associated with the observation. To apply this method, the variable you aim to predict must be categorical (like `subscription`). The data must adhere to the additional assumptions stated below to ensure the validity and accuracy of this model:\n",
    "\n",
    "* **Linearity**: Logistic regression, which is used in LDA, assumes a linear relationship between the natural log of probabilities (expressed as odds) and predictor variables.\n",
    "\n",
    "* **No Outliers: Outliers**: Data points with exceptionally large or small values, should not be present in the variables of interest.\n",
    "\n",
    "* **Independence**: Each observation or data point should be independent of others. In cases where multiple data points are collected over time from the same unit of observation, this assumption may be violated due to potential relatedness or influence among the data points.\n",
    "\n",
    "* **No Multicollinearity**: Multicollinearity occurs when two or more independent variables are highly correlated. This can lead to instability and reduced reliability of regression coefficients and statistical significance, although it does not affect the overall fit of the model.\n",
    "\n",
    "* **Homoscedasticity**: This means that the variability of the variables should be consistent throughout their respective ranges.\n",
    "\n",
    "* **Normality**: Assumes the distributions of the variables follow a normal (bell curve) distribution shape.\n",
    "\n",
    "In the following section, Quadratic Discriminant Analysis (QDA) will be discussed in further detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, power_transform, PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis,\n",
    "    QuadraticDiscriminantAnalysis,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the CSV files for train, validation, and test\n",
    "X_train = pd.read_csv('0_X_train.csv', index_col = 'Id')\n",
    "X_valid = pd.read_csv('1_X_valid.csv', index_col = 'Id')\n",
    "X_test  = pd.read_csv('2_X_test.csv', index_col = 'Id')\n",
    "\n",
    "y_train = pd.read_csv('0_y_train.csv', index_col = 'Id')\n",
    "y_valid = pd.read_csv('1_y_valid.csv', index_col = 'Id')\n",
    "y_test  = pd.read_csv('2_y_test.csv', index_col = 'Id')\n",
    "\n",
    "X = pd.concat([X_train, X_valid, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_valid, y_test], axis=0)\n",
    "\n",
    "num_vars = ['age', 'time_spent', 'banner_views', 'banner_views_old', 'days_elapsed_old', 'X4']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately, our possible models for LDA and QDA are reduced since we can only use continuous variables to predict the response, $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the continuous variables for LDA and QDA\n",
    "X_train = X_train[['age', 'time_spent', 'banner_views', 'banner_views_old', 'days_elapsed_old', 'X4']]\n",
    "\n",
    "X_valid = X_valid[['age', 'time_spent', 'banner_views', 'banner_views_old', 'days_elapsed_old', 'X4']]\n",
    "\n",
    "X_test = X_test[['age', 'time_spent', 'banner_views', 'banner_views_old', 'days_elapsed_old', 'X4']]\n",
    "\n",
    "X = X[['age', 'time_spent', 'banner_views', 'banner_views_old', 'days_elapsed_old', 'X4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for only continuous predictors\n",
    "corr_matrix = X_train_continous.corr()\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize = (10, 10), dpi = 600)\n",
    "g = sns.heatmap(corr_matrix, square = True,\n",
    "            center = 0, annot = True, linewidths = .5,\n",
    "            cmap = \"RdBu_r\", cbar_kws = {\"shrink\": 0.8}, vmin = -1, vmax = 1);\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation = 45, fontsize = 8, ha = 'right')\n",
    "plt.title('Correlation Matrix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest correlation between variables is `days_elapsed_old` and `banner_views_old` at 50%, possibly indicating a strong relatonship. This could hinder the independence assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array for y_train so that the methods can appropriately read the data\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# Create a numpy array for y_train so that the methods can appropriately read the data\n",
    "y = np.array(y)\n",
    "y = y.ravel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the pipeline ...\n",
    "# Define pipeline\n",
    "pipe_lda = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), \n",
    "     (\"lda\", LinearDiscriminantAnalysis())]\n",
    ")\n",
    "# Pipeline consists of two steps (treating as a single object):\n",
    "#  1. StandardScaler: This step applies the standard scaling technique to the input features. \n",
    "#  Standard scaling scales each feature to have zero mean and unit variance, which is a common preprocessing step for many machine learning algorithms.\n",
    "#  2. LinearDiscriminantAnalysis: Performs linear discriminant analysis, a technique for dimensionality reduction and classification. \n",
    "#  It aims to find a lower-dimensional representation of the data that maximizes the separation between different classes in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Best Model\n",
      "\n",
      "Training accuracy: 0.748\n",
      "Validation accuracy: 0.7297\n",
      "Test accuracy: 0.7506\n",
      "X accuracy for Partially Trained Model: 0.7456\n",
      "X accuracy on Fully Trained Model: 0.747\n"
     ]
    }
   ],
   "source": [
    "# Fit pipeline\n",
    "pipe_lda.fit(X_train, y_train) # Fit it to the training data \n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = pipe_lda.predict(X_train) # Predict the class labels for the training data\n",
    "acc1 = accuracy_score(y_train, y_train_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "# Predict on valid data\n",
    "y_valid_pred = pipe_lda.predict(X_valid) # Predict the class labels for the training data\n",
    "acc2 = accuracy_score(y_valid, y_valid_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = pipe_lda.predict(X_test) # Predict the class labels for the test data\n",
    "acc3 = accuracy_score(y_test, y_test_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "# Predict pipeline to X training (full) data\n",
    "y_X_pred = pipe_lda.predict(X) # Predict the class labels for the X data\n",
    "acc4 = accuracy_score(y, y_X_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "# Fit pipeline to full training X data\n",
    "pipe_lda.fit(X, y) # Fit it to the full X training data \n",
    "\n",
    "# Predict pipeline to X training (full) data\n",
    "y_XX_pred = pipe_lda.predict(X) # Predict the class labels for the X data\n",
    "acc5 = accuracy_score(y, y_XX_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "print(\"LDA Best Model\")\n",
    "print(\"\")\n",
    "print(\"Training accuracy:\",  np.round(acc1, 4))\n",
    "print(\"Validation accuracy:\", np.round(acc2, 4))\n",
    "print(\"Test accuracy:\", np.round(acc3, 4))\n",
    "print(\"X accuracy on Partially Trained Model:\", np.round(acc4, 4))\n",
    "print(\"X accuracy on Fully Trained Model:\", np.round(acc5, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis \n",
    "The extension of LDA is QDA, which differs in the fact that this method assumes different covariance matrices for each class. Instead of linear decision boundaries like LDA had, we now use quadratic questions in $x$. This allows for more flexiblity. However, there are more parameters to estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('qda', QuadraticDiscriminantAnalysis())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QDA\n",
    "# A classification algorithm that assumes the data follows a Gaussian distribution and estimates separate covariance matrices for each class. \n",
    "# It is a type of discriminant analysis that is particularly useful when the classes have non-linear decision boundaries.\n",
    "\n",
    "# Define pipeline\n",
    "pipe_qda = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), \n",
    "     (\"qda\", QuadraticDiscriminantAnalysis())]\n",
    ")\n",
    "# Pipeline consists of two steps (treating as a single object):\n",
    "#  1. StandardScaler: This step applies the standard scaling technique to the input features. \n",
    "#  Standard scaling scales each feature to have zero mean and unit variance, which is a common preprocessing step for many machine learning algorithms.\n",
    "#  2. QuadraticDiscriminantAnalysis: This step fits a QDA model to the scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Best Model\n",
      "\n",
      "Training accuracy: 0.7244\n",
      "Validation accuracy: 0.7081\n",
      "Test accuracy: 0.7036\n",
      "X accuracy for Partially Trained Model: 0.7188\n",
      "X accuracy on Fully Trained Model: 0.724\n"
     ]
    }
   ],
   "source": [
    "# Fit pipeline\n",
    "pipe_qda.fit(X_train, y_train) # Fit it to the training data \n",
    "\n",
    "# Predict on training data\n",
    "y_train_pred = pipe_qda.predict(X_train) # Predict the class labels for the training data\n",
    "acc1 = accuracy_score(y_train, y_train_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "# Predict on valid data\n",
    "y_valid_pred = pipe_qda.predict(X_valid) # Predict the class labels for the training data\n",
    "acc2 = accuracy_score(y_valid, y_valid_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred = pipe_qda.predict(X_test) # Predict the class labels for the test data\n",
    "acc3 = accuracy_score(y_test, y_test_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "# Predict pipeline to X training (full) data\n",
    "y_X_pred = pipe_qda.predict(X) # Predict the class labels for the X data\n",
    "acc4 = accuracy_score(y, y_X_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "# Fit pipeline to full training X data\n",
    "pipe_qda.fit(X, y) # Fit it to the full X training data \n",
    "\n",
    "# Predict pipeline to X training (full) data\n",
    "y_XX_pred = pipe_qda.predict(X) # Predict the class labels for the X data\n",
    "acc5 = accuracy_score(y, y_XX_pred) # Calculate the accuracy of the predictions \n",
    "\n",
    "print(\"QDA Best Model\")\n",
    "print(\"\")\n",
    "print(\"Training accuracy:\",  np.round(acc1, 4))\n",
    "print(\"Validation accuracy:\", np.round(acc2, 4))\n",
    "print(\"Test accuracy:\", np.round(acc3, 4))\n",
    "print(\"X accuracy on Partially Trained Model:\", np.round(acc4, 4))\n",
    "print(\"X accuracy on Fully Trained Model:\", np.round(acc5, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "* https://www.datasklr.com/select-classification-methods/linear-and-quadratic-discriminant-analysis\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html \n",
    "\n",
    "* https://online.stat.psu.edu/stat508/lesson/9/9.2/9.2.8\n",
    "\n",
    "* https://www.statstest.com/linear-discriminant-analysis/\n",
    "\n",
    "* https://towardsdatascience.com/linear-discriminant-analysis-explained-f88be6c1e00b \n",
    "\n",
    "* 10_LDA_and_QDA.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
