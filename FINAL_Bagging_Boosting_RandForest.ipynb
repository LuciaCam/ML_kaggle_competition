{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging, Boosting and Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are already quite flexible but they are limited by how the splits are selected. The *greedy algorithm* of selecting the best split for the current node can affect the performance of nodes lower in the tree. Due to this some enhancements have been made to these to help in the bias-variance tradeoff."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging stands for \"*bootstrap aggregation*\" which consists of using bootstrap sampling to grow various trees with a partial sample of the data to then average the results of each tree. This is particularly helpful to reduce variance in predictions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting consists of using many \"weak\" small trees with very *low variance* but to add them together so that the *bias* can be reduced. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Random forests are the growing of multiple trees but changing the variable selected to do the split every time so there is no bias from the *greedy algorithm*. We proceed to fit all 3 decision tree enhancements in this section."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from yellowbrick.classifier import classification_report, confusion_matrix\n",
    "from yellowbrick.classifier.rocauc import roc_auc\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('0_X_train.csv', index_col='Id')\n",
    "X_valid = pd.read_csv('1_X_valid.csv', index_col='Id')\n",
    "X_test  = pd.read_csv('2_X_test.csv', index_col='Id')\n",
    "\n",
    "y_train = pd.read_csv('0_y_train.csv', index_col='Id')\n",
    "y_valid = pd.read_csv('1_y_valid.csv', index_col='Id')\n",
    "y_test  = pd.read_csv('2_y_test.csv', index_col='Id')\n",
    "\n",
    "#full training set\n",
    "X = pd.concat([X_train, X_valid, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_valid, y_test], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the Y values to format correctly as per the  `scikitlearn` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "y = np.array(y)\n",
    "y = y.ravel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Bagged Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the sections, we proceed to find the parameters that generate the best model by comparing their prediction accuracy scores. \n",
    "\n",
    "There are a series of parameters we can adjust. `min_samples_leaf` is the final number of observations in a determined terminal node. `n_estimators` is the amount of trees to be fitted for each round of model generation. `max_features` here is set to none which indicates the function can automatically decide which parameters to split on. This parameter will be modified in Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB: 0.85174\n",
      "Best parameters: {'min_samples_leaf': 3, 'n_estimators': 2000}\n"
     ]
    }
   ],
   "source": [
    "bag_hyperparam_grid={'min_samples_leaf':[1, 3, 5, 7, 9,], \"n_estimators\":[500, 750, 1000, 1250, 1500, 2000,]}\n",
    "bag = RandomForestClassifier(oob_score=True, max_features=None, criterion=\"log_loss\",\n",
    "                             warm_start=False, random_state=1, n_jobs=-2)\n",
    "best_score=0.5\n",
    "\n",
    "for g in ParameterGrid(bag_hyperparam_grid):\n",
    "    bag.set_params(**g)\n",
    "    bag.fit(X_train,y_train)\n",
    "    # save if best\n",
    "    if bag.oob_score_ > best_score:\n",
    "        best_score = bag.oob_score_\n",
    "        best_params = g\n",
    "\n",
    "print(f\"OOB: %0.5f\" % best_score)\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "#Student note: takes over 7 mins to run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the best parameters have been found we can safe to a new variable and print the predictive accuracy of this model. For this data set the best parameters are: `min_samples_leaf` = 3, `n_estimators` = 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = RandomForestClassifier(min_samples_leaf = best_params[\"min_samples_leaf\"], \n",
    "                             n_estimators = best_params[\"n_estimators\"],oob_score=True, \n",
    "                             max_features=None, criterion=\"log_loss\",warm_start=False, \n",
    "                             random_state=1, n_jobs=-2)\n",
    "\n",
    "#create the bagged tree\n",
    "# bag = RandomForestClassifier(min_samples_leaf = 3, n_estimators = 200,oob_score=True, \n",
    "#                              max_features=None, criterion=\"log_loss\",warm_start=False, \n",
    "#                              random_state=1, n_jobs=-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Bagged Tree Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged tree Best Model\n",
      " \n",
      "Training accuracy: 0.9764\n",
      "Validation accuracy: 0.8362\n",
      "Test accuracy: 0.8474\n",
      "X accuracy for Partially Trained Model: 0.936\n",
      "X accuracy on Fully Trained Model: 0.9734\n"
     ]
    }
   ],
   "source": [
    "#Print the results\n",
    "print(\"Bagged tree Best Model\")\n",
    "print(\" \")\n",
    "\n",
    "#fit and predict on partitioned data set\n",
    "bag.fit(X_train,y_train)\n",
    "print(\"Training accuracy:\", round(bag.score(X_train, y_train),4))\n",
    "print(\"Validation accuracy:\", round(bag.score(X_valid, y_valid),4))\n",
    "print(\"Test accuracy:\", round(bag.score(X_test, y_test),4))\n",
    "print(\"X accuracy for Partially Trained Model:\", round(bag.score(X, y),4))\n",
    "\n",
    "#fit and predict on all data\n",
    "bag.fit(X,y)\n",
    "print(\"X accuracy on Fully Trained Model:\", round(bag.score(X, y),4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Boosted Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the boosted trees, an additional parameter is indicated. The learning rate ($\\lambda$) adjusts the size of the tree at a certain rate for this it is called a *slow learner*. The `learning_rate` is  avalue between 0 and 1 and in finding the best model we indicate various values of this new parameter. Additionally, `max_depth` changes the number of splits done in the tree. As seen from regular trees, the number of nodes affects the prediction accuracy on all sets, so finding the most appropriate one is also important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB: 1.00000\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 1250}\n"
     ]
    }
   ],
   "source": [
    "boost_hyperparam_grid={'max_depth':[1, 3, 5, 7, 9,], \n",
    "                       \"n_estimators\":[500, 750, 1000, 1250, 1500, 2000,], \n",
    "                       \"learning_rate\":[0.001, 0.01, 0.1, 0.5, 1]}\n",
    "\n",
    "boost = GradientBoostingClassifier(max_features=None, \n",
    "                             warm_start=False, random_state=1)\n",
    "\n",
    "best_boost_score=0.5\n",
    "\n",
    "for bg in ParameterGrid(boost_hyperparam_grid):\n",
    "    boost.set_params(**bg)\n",
    "    boost.fit(X_train,y_train)\n",
    "    # save if best\n",
    "    if boost.score(X_train, y_train) > best_boost_score:\n",
    "        best_boost_score = boost.score(X_train, y_train)\n",
    "        best_boost_params = bg\n",
    "\n",
    "print(f\"OOB: %0.5f\" % best_boost_score)\n",
    "print(\"Best parameters:\", best_boost_params)\n",
    "\n",
    "#Student note: takes over 55 mins to run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As so we can see that the best combination tha best maximizes the accuracy is  `learning_rate`= 0.01, `max_depth`= 9, and `n_estimators`= 1250.\n",
    "\n",
    "With this we train a new model and find the accuracy of the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = GradientBoostingClassifier(max_depth = best_boost_params[\"max_depth\"], \n",
    "                                  n_estimators = best_boost_params[\"n_estimators\"],\n",
    "                                  learning_rate = best_boost_params[\"learning_rate\"],\n",
    "                                  random_state=1)\n",
    "\n",
    "# boost = GradientBoostingClassifier(max_depth = 9, \n",
    "#                                    n_estimators = 1250,\n",
    "#                                    learning_rate = 0.01,\n",
    "#                                    random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Boosted Tree Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosted tree Best Model\n",
      " \n",
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.8392\n",
      "Test accuracy: 0.86\n",
      "X accuracy for Partially Trained Model: 0.9549\n",
      "X accuracy on Fully Trained Model: 0.9977\n"
     ]
    }
   ],
   "source": [
    "#Print the results\n",
    "print(\"Boosted tree Best Model\")\n",
    "print(\" \")\n",
    "\n",
    "#fit and predict on partitioned data set\n",
    "boost.fit(X_train,y_train)\n",
    "print(\"Training accuracy:\", round(boost.score(X_train, y_train),4))\n",
    "print(\"Validation accuracy:\", round(boost.score(X_valid, y_valid),4))\n",
    "print(\"Test accuracy:\", round(boost.score(X_test, y_test),4))\n",
    "print(\"X accuracy for Partially Trained Model:\", round(boost.score(X, y),4))\n",
    "\n",
    "#fit and predict on all data\n",
    "boost.fit(X,y)\n",
    "print(\"X accuracy on Fully Trained Model:\", round(boost.score(X, y),4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For matters of comparison, we now run the best boosted model, as it has had a very good performance so far, with the predictors selected from Lasso variable selection to see if there is any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish unrequired variable names\n",
    "cols_lasso = ['banner_views_old', 'days_elapsed_old', 'X3', 'marital_divorced', 'job_entrepreneur', 'job_freelance',\n",
    "             'job_housekeeper', 'job_technology', 'job_unemployed']\n",
    "\n",
    "#create new partitions with corresponding lasso variables\n",
    "x_train_mod = X_train.drop(columns=cols_lasso)\n",
    "x_valid_mod = X_valid.drop(columns=cols_lasso)\n",
    "x_test_mod = X_test.drop(columns=cols_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9998\n",
      "Validation acccuracy: 0.8295\n",
      "Test accuracy: 0.8511\n"
     ]
    }
   ],
   "source": [
    "#fit the model on new training data\n",
    "boost.fit(x_train_mod, y_train)\n",
    "\n",
    "#show results of model\n",
    "print(\"Training accuracy:\", round(boost.score(x_train_mod, y_train),4))\n",
    "print(\"Validation acccuracy:\", round(boost.score(x_valid_mod, y_valid),4))\n",
    "print(\"Test accuracy:\", round(boost.score(x_test_mod, y_test),4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is no improvement from full variable boosting and the variable selection. Now we proceed to the same procedure with Random Forests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new parameter to choose in Random Forest is the number of features. This refers to the number of variables used in the random selection for growing new trees. To establish an appropopriate amount, rule of thumb is to start with the $\\sqrt{p}$, where $p$ is the number of predictors in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5677643628300215"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_max_features_param = np.sqrt(X_train.shape[1])\n",
    "default_max_features_param"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in this case its close to 5.5 we go two units below 5 and 2 units above 6 to have a decent set to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB: 0.84583\n",
      "Best parameters: {'max_features': 8, 'min_samples_leaf': 9, 'n_estimators': 2000}\n"
     ]
    }
   ],
   "source": [
    "#set hyperparameter grid\n",
    "rf_hyperparam_grid={\"max_features\":[3, 4, 5, 6, 7, 8],\n",
    "                 'min_samples_leaf':[1, 3, 5, 7, 9],\n",
    "                 \"n_estimators\":[500, 750, 1000, 1250, 1500, 2000]}\n",
    "\n",
    "#instantiate the randfortest\n",
    "rfc = RandomForestClassifier(criterion = \"log_loss\", oob_score=True, warm_start=False, random_state=1, n_jobs=-2)\n",
    "best_score=0.5\n",
    "\n",
    "#loop over parameters. running duration 24mins.\n",
    "for rfg in ParameterGrid(rf_hyperparam_grid):\n",
    "    rfc.set_params(**rfg)\n",
    "    rfc.fit(X_train,y_train)\n",
    "    # save if best\n",
    "    if rfc.oob_score_ > best_score:\n",
    "        best_rfc_score = rfc.oob_score_\n",
    "        best_rfc_params = rfg\n",
    "\n",
    "#print best results\n",
    "print(f\"OOB: %0.5f\" % best_rfc_score)\n",
    "print(\"Best parameters:\", best_rfc_params)\n",
    "\n",
    "#Student note: takes over 30 mins to run"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best combination is `max_features` = 7, `min_samples_leaf`= 9, and `n_estimators`= 2000. Now the final model is saved to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc = RandomForestClassifier(n_estimators=best_rfc_params['max_features'], \n",
    "#                              max_features=best_rfc_params['max_features'],\n",
    "#                              min_samples_leaf=best_rfc_params['min_samples_leaf'], \n",
    "#                              criterion=\"log_loss\",oob_score=True, warm_start=False, random_state=1)\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=2000, \n",
    "                             max_features=7,\n",
    "                             min_samples_leaf=9, \n",
    "                             criterion=\"log_loss\",oob_score=True, warm_start=False, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Random Forest Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Best Model\n",
      " \n",
      "Training accuracy: 0.8438\n",
      "Validation accuracy: 0.8258\n",
      "Test accuracy: 0.8496\n",
      "X accuracy for Partially Trained Model: 0.873\n",
      "X accuracy on Fully Trained Model: 0.8859\n"
     ]
    }
   ],
   "source": [
    "#Print the results\n",
    "print(\"Random Forest Best Model\")\n",
    "print(\" \")\n",
    "\n",
    "#fit and predict on partitioned data set\n",
    "rfc.fit(X_train,y_train)\n",
    "print(\"Training accuracy:\", round(rfc.oob_score_,4))\n",
    "print(\"Validation accuracy:\", round(rfc.score(X_valid, y_valid),4))\n",
    "print(\"Test accuracy:\", round(rfc.score(X_test, y_test),4))\n",
    "print(\"X accuracy for Partially Trained Model:\", round(rfc.score(X, y),4))\n",
    "\n",
    "#fit and predict on all data\n",
    "rfc.fit(X,y)\n",
    "print(\"X accuracy on Fully Trained Model:\", round(rfc.score(X, y),4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see that despite the result being better than the original tree, it did not improve upon the boosting results obtained previously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
