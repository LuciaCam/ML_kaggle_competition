{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Machine Vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Support Vector Machines (SVM) are widely recognized and widely used in for classification tasks. SVM offers distinct advantages compared to other methods, such as Linear Discriminant Analysis (LDA) and logistic regression. LDA focuses on modeling the joint distribution of both the target variable $(Y)$ and the input features $(X)$. On the other hand, logistic regression primarily models the conditional distribution of the target variable $(Y)$ given the input features $(X)$.\n",
    "\n",
    "In contrast, SVM takes a geometric approach and does not rely on any specific assumptions about the underlying distribution of the data. Instead, SVM tries to find an optimal separating hyperplane that effectively separates the different classes. By directly seeking the separating hyperplane, SVM provides a unique perspective in classification tasks and offers a powerful tool for solving classification problems. Its ability to handle complex data distributions without making explicit distributional assumptions makes SVM a versatile and widely adopted technique."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of classifiers, but we will specifically focus on one since the classes are not linearly separable as is assumed in the Maximal Margin Classifier anad Support Vector Classifier. Instead, we will utilize the Support Vector Machines that have a soft non-linear separation in the original feature space since these classes are not separable. In addition, we will review different kernels such as polynomial, RBF, and sigmoid since they can potentially be useful when the observations are not linearly separable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from libsvm.svmutil import *\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('0_X_train.csv', index_col='Id')\n",
    "X_valid = pd.read_csv('1_X_valid.csv', index_col='Id')\n",
    "X_test  = pd.read_csv('2_X_test.csv', index_col='Id')\n",
    "y_train = pd.read_csv('0_y_train.csv', index_col='Id')\n",
    "y_valid = pd.read_csv('1_y_valid.csv', index_col='Id')\n",
    "y_test  = pd.read_csv('2_y_test.csv', index_col='Id')\n",
    "\n",
    "X = pd.concat([X_train, X_valid, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_valid, y_test], axis=0)\n",
    "\n",
    "num_vars = ['age', 'time_spent', 'banner_views', 'banner_views_old', 'days_elapsed_old', 'X4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array for y_train so that the methods can appropriately read the data\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# Create a numpy array for y_train so that the methods can appropriately read the data\n",
    "y = np.array(y)\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype('float')\n",
    "y_train = np.asarray(y_train).ravel()\n",
    "\n",
    "X = np.asarray(X).astype('float')\n",
    "y = np.asarray(y).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = np.asarray(X_valid).astype('float')\n",
    "y_valid = np.asarray(y_valid).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.asarray(X_test).astype('float')\n",
    "y_test = np.asarray(y_test).ravel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Polynomial Kernel\n",
    "SVM with a polynomial kernel applies a polynomial function to map the input data into a higher-dimensional feature space, allowing for non-linear decision boundaries.\n",
    "\n",
    "The polynomial kernel function is defined as $k(x, y) = (x · y + c)^d$, where $x$ and $y$ are the input feature vectors, $·$ represents the dot product, $c$ is a constant term, and $d$ is the degree of the polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-q')\n",
    "param.kernel_type = 1\n",
    "problem = svm_problem(y_train, X_train)\n",
    "model1 = svm_train(problem, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 65.0335% (4075/6266) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_train, X_train, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 63.589% (854/1343) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_valid, X_valid, model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 65.9717% (886/1343) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_test, X_test, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 64.9576% (5815/8952) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y, X, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-q')\n",
    "param.kernel_type = 1\n",
    "problem = svm_problem(y, X)\n",
    "model2 = svm_train(problem, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 64.3878% (5764/8952) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y, X, model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Radial Basis Function Kernel\n",
    "\n",
    "RBF kernel is effective for dealing with non-linearly separable data because it allows this model to capture complex decision boundaries without explicitly mapping the data into a higher-dimensional feature space. \n",
    "\n",
    "The RBF kernel function is defined as $k(x, y) = exp(-\\gamma * ||x - y||^2)$, where $x$ and $y$ are the input feature vectors, $||x - y||$ represents the Euclidean distance between $x$ and $y$, and $\\gamma$ is a hyperparameter that controls the shape of the decision boundary. A smaller value of $\\gamma$ will lead to a softer decision boundary, while a larger value makes the boundary more rigid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-q')\n",
    "param.kernel_type = 2\n",
    "problem = svm_problem(y_train, X_train)\n",
    "model3 = svm_train(problem, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 91.1586% (5712/6266) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_train, X_train, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 79.8213% (1072/1343) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_valid, X_valid, model3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 80.0447% (1075/1343) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_test, X_test, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 87.7904% (7859/8952) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y, X, model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-q')\n",
    "param.kernel_type = 2\n",
    "problem = svm_problem(y, X)\n",
    "model4 = svm_train(problem, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 90.8847% (8136/8952) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y, X, model4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Sigmoid Kernel\n",
    "\n",
    "The sigmoid kernel is derived from the sigmoid activation function which is also commonly used in neural networks. It is defined as $k(x, y) = tanh(\\alpha*x^Ty + c)$, where $x$ and $y$ are the input feature vectors, $\\alpha$ is a hyperparameter that controls the shape of the decision boundary, and $c$ is another hyperparameter that controls the bias term.\n",
    "\n",
    "Compared to other kernel functions like the RBF kernel above, the sigmoid kernel tends to produce decision boundaries that are less smooth and more prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-q')\n",
    "param.kernel_type = 3\n",
    "problem = svm_problem(y_train, X_train)\n",
    "model5 = svm_train(problem, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 58.3307% (3655/6266) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_train, X_train, model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 57.7811% (776/1343) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_valid, X_valid, model5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 59.0469% (793/1343) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y_test, X_test, model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 58.3557% (5224/8952) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y, X, model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-q')\n",
    "param.kernel_type = 3\n",
    "problem = svm_problem(y, X)\n",
    "model6 = svm_train(problem, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 58.3557% (5224/8952) (classification)\n"
     ]
    }
   ],
   "source": [
    "pred_lbl, pred_acc, pred_val = svm_predict(y, X, model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Best Model\n",
      "\n",
      "Training accuracy: 0.9115\n",
      "Validation accuracy: 0.7982\n",
      "Test accuracy: 0.8004\n",
      "X accuracy on Partially Trained Model: 0.8779\n",
      "X accuracy on Fully Trained Model: 0.9088\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Best Model\")\n",
    "print(\"\")\n",
    "print(\"Training accuracy:\",  np.round(.9115, 4))\n",
    "print(\"Validation accuracy:\", np.round(.798213, 4))\n",
    "print(\"Test accuracy:\", np.round(.800447, 4))\n",
    "print(\"X accuracy on Partially Trained Model:\", np.round(.877904, 4))\n",
    "print(\"X accuracy on Fully Trained Model:\", np.round(.908847, 4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://dataaspirant.com/svm-kernels/#t-1608054630728\n",
    "\n",
    "* https://data-flair.training/blogs/svm-kernel-functions/\n",
    "\n",
    "* 18_SVM_MMC.pdf\n",
    "\n",
    "* 20_SVM.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
